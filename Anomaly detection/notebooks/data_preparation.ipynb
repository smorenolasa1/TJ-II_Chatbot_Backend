{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Activate the virtual environment: \n",
    "source /Users/sofiamorenolasa/Desktop/TFG/.venv/bin/activate\n",
    "2. Switch to the virtual environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'ascii' codec can't decode byte 0xd3 in position 15104: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/PARAMETROS_TJ2.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Read the CSV with ASCII encoding\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m;\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mascii\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Display the first few rows to confirm it loaded successfully\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(data\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[0;32m~/Desktop/TFG/.venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/TFG/.venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Desktop/TFG/.venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/TFG/.venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/TFG/.venv/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mparsers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[0;32mparsers.pyx:574\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:663\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0xd3 in position 15104: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "# Importar pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Importar chardet to help with encoding in UTF-8\n",
    "import chardet\n",
    "\n",
    "# Path del archivo\n",
    "file_path = \"../data/PARAMETROS_TJ2.csv\"\n",
    "\n",
    "# Read the CSV with ASCII encoding\n",
    "data = pd.read_csv(file_path, delimiter=\";\", encoding=\"ascii\")\n",
    "\n",
    "# Display the first few rows to confirm it loaded successfully\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   N_DESCARGA       fecha   hora comentarioDesc comentarioExp configuracion  \\\n",
      "0         112  19971217.0  19:05            NaN           NaN           NaN   \n",
      "1         113  19971217.0  19:09            NaN           NaN           NaN   \n",
      "2         114  19971217.0  19:55            NaN           NaN           NaN   \n",
      "3         115  19971218.0  11:08            NaN           NaN           NaN   \n",
      "4         116  19971218.0  11:28            NaN           NaN           NaN   \n",
      "\n",
      "   potencia_radiada  energia_diamagnetica  retraso_densidad_girotron  zeff  \\\n",
      "0               NaN                   NaN                        NaN   NaN   \n",
      "1               NaN                   NaN                        NaN   NaN   \n",
      "2               NaN                   NaN                        NaN   NaN   \n",
      "3               NaN                   NaN                        NaN   NaN   \n",
      "4               NaN                   NaN                        NaN   NaN   \n",
      "\n",
      "   ...  IAccel_nominal_NBI2  tini_NBI2  longitud_pulso_nominal_NBI2  \\\n",
      "0  ...                  NaN        NaN                          NaN   \n",
      "1  ...                  NaN        NaN                          NaN   \n",
      "2  ...                  NaN        NaN                          NaN   \n",
      "3  ...                  NaN        NaN                          NaN   \n",
      "4  ...                  NaN        NaN                          NaN   \n",
      "\n",
      "   potencia_nominal_NBI2  potencia_through_port_NBI2  VAccel_real_NBI2  \\\n",
      "0                    NaN                         NaN               NaN   \n",
      "1                    NaN                         NaN               NaN   \n",
      "2                    NaN                         NaN               NaN   \n",
      "3                    NaN                         NaN               NaN   \n",
      "4                    NaN                         NaN               NaN   \n",
      "\n",
      "   IAccel_real_NBI2  longitud_pulso_real_NBI2  updated_NBI2  \\\n",
      "0               NaN                       NaN           NaN   \n",
      "1               NaN                       NaN           NaN   \n",
      "2               NaN                       NaN           NaN   \n",
      "3               NaN                       NaN           NaN   \n",
      "4               NaN                       NaN           NaN   \n",
      "\n",
      "   factor_transm_NBI2  \n",
      "0                 NaN  \n",
      "1                 NaN  \n",
      "2                 NaN  \n",
      "3                 NaN  \n",
      "4                 NaN  \n",
      "\n",
      "[5 rows x 155 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b6/pb_stfjn0pb7pybgwlqjk5700000gn/T/ipykernel_87239/2676448733.py:6: DtypeWarning: Columns (52,55,96,132,133,144,145) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(file_path, delimiter=\";\", encoding=\"latin1\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"../data/PARAMETROS_TJ2.csv\"\n",
    "\n",
    "# Use latin1 encoding\n",
    "data = pd.read_csv(file_path, delimiter=\";\", encoding=\"latin1\")\n",
    "\n",
    "# Display the first few rows\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we sort the PARAMETROS_TJ2.csv file by the N_DESCARGA field and save the sorted data to a new file named PARAMETROS_TJ2_ORDENADOS.csv. This ensures the rows are ordered starting from the smallest N_DESCARGA value to the largest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b6/pb_stfjn0pb7pybgwlqjk5700000gn/T/ipykernel_87239/1307042932.py:8: DtypeWarning: Columns (52,55,96,132,133,144,145) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(input_file_path, delimiter=\";\", encoding=\"latin1\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted data saved to ../data/PARAMETROS_TJ2_ORDENADOS.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "input_file_path = \"../data/PARAMETROS_TJ2.csv\"  # Replace with the correct path\n",
    "output_file_path = \"../data/PARAMETROS_TJ2_ORDENADOS.csv\"\n",
    "\n",
    "# Read the data\n",
    "data = pd.read_csv(input_file_path, delimiter=\";\", encoding=\"latin1\")\n",
    "\n",
    "# Convert N_DESCARGA to numeric, coercing errors to NaN\n",
    "data['N_DESCARGA'] = pd.to_numeric(data['N_DESCARGA'], errors='coerce')\n",
    "\n",
    "# Drop rows where N_DESCARGA is NaN (invalid values)\n",
    "data = data.dropna(subset=['N_DESCARGA'])\n",
    "\n",
    "# Ensure N_DESCARGA is an integer\n",
    "data['N_DESCARGA'] = data['N_DESCARGA'].astype(int)\n",
    "\n",
    "# Sort by N_DESCARGA in ascending order\n",
    "data_sorted = data.sort_values(by='N_DESCARGA')\n",
    "\n",
    "# Save the sorted dataset to a new CSV file\n",
    "data_sorted.to_csv(output_file_path, index=False, sep=\";\", encoding=\"latin1\")\n",
    "\n",
    "print(f\"Sorted data saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data saved to ../data/llm_training_data.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Initialize an empty list to store JSONL entries\n",
    "jsonl_data = []\n",
    "\n",
    "# Example 1: Filter by date range and include comentarioDesc\n",
    "date_range_start = 20210610\n",
    "date_range_end = 20210611\n",
    "filtered_by_date = data[(data[\"fecha\"] >= date_range_start) & (data[\"fecha\"] <= date_range_end)]\n",
    "\n",
    "prompt_1 = \"Dame una lista de los comentarios que había entre el 20210610 y el 20210616.\"\n",
    "completion_1 = \"\\n\".join(\n",
    "    [f\"- Descarga {row['N_DESCARGA']}: Comentario: {row['comentarioDesc']}\" for _, row in filtered_by_date.iterrows()]\n",
    ")\n",
    "jsonl_data.append({\"prompt\": prompt_1, \"completion\": completion_1})\n",
    "\n",
    "# Example 2: Filter by a threshold on a specific column\n",
    "threshold = 56\n",
    "filtered_by_threshold = data[data[\"posicion_sonda_d4top\"] > threshold]\n",
    "\n",
    "prompt_2 = \"En qué descargas hubo una posición de la sonda mayor a 50?\"\n",
    "completion_2 = \"Descargas: \" + \", \".join(map(str, filtered_by_threshold[\"N_DESCARGA\"].tolist()))\n",
    "jsonl_data.append({\"prompt\": prompt_2, \"completion\": completion_2})\n",
    "\n",
    "\n",
    "# Save the generated training data to a JSONL file\n",
    "output_path = \"../data/llm_training_data.jsonl\"\n",
    "with open(output_path, \"w\") as f:\n",
    "    for entry in jsonl_data:\n",
    "        f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Training data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONL file saved as PARAMETROS_TJ2_training_data.jsonl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the data from CSV\n",
    "data_path = \"../data/PARAMETROS_TJ2.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Replace NaN values with defaults\n",
    "df = df.fillna({\"energia_diamagnetica\": \"Unknown\", \"duracion_programada\": \"Unknown\", \"duracion_real\": \"Unknown\", \"potencia_radiada\": 0})\n",
    "\n",
    "# Initialize JSONL storage\n",
    "jsonl_data = []\n",
    "\n",
    "# Example 1: Retrieve comments within a date range\n",
    "date_range_start = 20210610\n",
    "date_range_end = 20210611\n",
    "filtered_by_date = df[(df[\"fecha\"] >= date_range_start) & (df[\"fecha\"] <= date_range_end)]\n",
    "\n",
    "prompt_1 = \"Dame una lista de los comentarios que había entre el 20210610 y el 20210611.\"\n",
    "completion_1 = \"\\n\".join(\n",
    "    [f\"- Descarga {row['N_DESCARGA']}: {row['comentarioDesc']}\" for _, row in filtered_by_date.iterrows()]\n",
    ") or \"No hay comentarios en este rango de fechas.\"\n",
    "jsonl_data.append({\"prompt\": prompt_1, \"completion\": completion_1})\n",
    "\n",
    "# Example 2: Check for gas failures in a specific discharge\n",
    "discharge_id = 1002  # Replace with a real ID from your data\n",
    "discharge_data = df[df[\"N_DESCARGA\"] == discharge_id]\n",
    "if not discharge_data.empty:\n",
    "    discharge_data = discharge_data.iloc[0]\n",
    "    prompt_2 = f\"¿Hubo fallos de gas durante la descarga {discharge_id}?\"\n",
    "    completion_2 = \"Sí, hubo un fallo de gas.\" if discharge_data[\"fallo_gas\"] else \"No, no hubo fallos de gas.\"\n",
    "    jsonl_data.append({\"prompt\": prompt_2, \"completion\": completion_2})\n",
    "\n",
    "# Example 3: Compare energy values between two discharges\n",
    "discharge_a = 1001  # Replace with real IDs\n",
    "discharge_b = 1002\n",
    "data_a = df[df[\"N_DESCARGA\"] == discharge_a]\n",
    "data_b = df[df[\"N_DESCARGA\"] == discharge_b]\n",
    "\n",
    "if not data_a.empty and not data_b.empty:\n",
    "    data_a = data_a.iloc[0]\n",
    "    data_b = data_b.iloc[0]\n",
    "    prompt_3 = f\"¿Qué descarga tuvo más energía diamagnética, la {discharge_a} o la {discharge_b}?\"\n",
    "    if data_a[\"energia_diamagnetica\"] > data_b[\"energia_diamagnetica\"]:\n",
    "        completion_3 = f\"La descarga {discharge_a} tuvo más energía diamagnética ({data_a['energia_diamagnetica']} J).\"\n",
    "    else:\n",
    "        completion_3 = f\"La descarga {discharge_b} tuvo más energía diamagnética ({data_b['energia_diamagnetica']} J).\"\n",
    "    jsonl_data.append({\"prompt\": prompt_3, \"completion\": completion_3})\n",
    "\n",
    "# Example 4: List discharges with radiated power above a threshold\n",
    "threshold = 300\n",
    "high_power_discharges = df[df[\"potencia_radiada\"] > threshold]\n",
    "\n",
    "prompt_4 = f\"Dame las descargas con potencia radiada mayor que {threshold}.\"\n",
    "completion_4 = \"\\n\".join(\n",
    "    [f\"- Descarga {row['N_DESCARGA']}: Potencia radiada {row['potencia_radiada']} W\" for _, row in high_power_discharges.iterrows()]\n",
    ") or \"No hay descargas con potencia radiada mayor que el umbral.\"\n",
    "jsonl_data.append({\"prompt\": prompt_4, \"completion\": completion_4})\n",
    "\n",
    "# Example 5: Check the programmed and real durations of a specific discharge\n",
    "if not discharge_data.empty:\n",
    "    prompt_5 = f\"¿Cuál fue la duración programada y la real de la descarga {discharge_id}?\"\n",
    "    completion_5 = f\"La duración programada fue de {discharge_data['duracion_programada']} ms y la real fue de {discharge_data['duracion_real']} ms.\"\n",
    "    jsonl_data.append({\"prompt\": prompt_5, \"completion\": completion_5})\n",
    "\n",
    "# Save to JSONL file\n",
    "output_file = \"PARAMETROS_TJ2_training_data.jsonl\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in jsonl_data:\n",
    "        json.dump(entry, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"JSONL file saved as {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
