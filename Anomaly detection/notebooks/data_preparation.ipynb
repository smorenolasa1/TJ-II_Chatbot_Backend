{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Activate the virtual environment: \n",
    "source /Users/sofiamorenolasa/Desktop/TFG/.venv/bin/activate\n",
    "2. Switch to the virtual environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   N_DESCARGA       fecha   hora comentarioDesc comentarioExp configuracion  \\\n",
      "0         112  19971217.0  19:05            NaN           NaN           NaN   \n",
      "1         113  19971217.0  19:09            NaN           NaN           NaN   \n",
      "2         114  19971217.0  19:55            NaN           NaN           NaN   \n",
      "3         115  19971218.0  11:08            NaN           NaN           NaN   \n",
      "4         116  19971218.0  11:28            NaN           NaN           NaN   \n",
      "\n",
      "   potencia_radiada  energia_diamagnetica  retraso_densidad_girotron  zeff  \\\n",
      "0               NaN                   NaN                        NaN   NaN   \n",
      "1               NaN                   NaN                        NaN   NaN   \n",
      "2               NaN                   NaN                        NaN   NaN   \n",
      "3               NaN                   NaN                        NaN   NaN   \n",
      "4               NaN                   NaN                        NaN   NaN   \n",
      "\n",
      "   ...  IAccel_nominal_NBI2  tini_NBI2  longitud_pulso_nominal_NBI2  \\\n",
      "0  ...                  NaN        NaN                          NaN   \n",
      "1  ...                  NaN        NaN                          NaN   \n",
      "2  ...                  NaN        NaN                          NaN   \n",
      "3  ...                  NaN        NaN                          NaN   \n",
      "4  ...                  NaN        NaN                          NaN   \n",
      "\n",
      "   potencia_nominal_NBI2  potencia_through_port_NBI2  VAccel_real_NBI2  \\\n",
      "0                    NaN                         NaN               NaN   \n",
      "1                    NaN                         NaN               NaN   \n",
      "2                    NaN                         NaN               NaN   \n",
      "3                    NaN                         NaN               NaN   \n",
      "4                    NaN                         NaN               NaN   \n",
      "\n",
      "   IAccel_real_NBI2  longitud_pulso_real_NBI2  updated_NBI2  \\\n",
      "0               NaN                       NaN           NaN   \n",
      "1               NaN                       NaN           NaN   \n",
      "2               NaN                       NaN           NaN   \n",
      "3               NaN                       NaN           NaN   \n",
      "4               NaN                       NaN           NaN   \n",
      "\n",
      "   factor_transm_NBI2  \n",
      "0                 NaN  \n",
      "1                 NaN  \n",
      "2                 NaN  \n",
      "3                 NaN  \n",
      "4                 NaN  \n",
      "\n",
      "[5 rows x 155 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b6/pb_stfjn0pb7pybgwlqjk5700000gn/T/ipykernel_87239/2676448733.py:6: DtypeWarning: Columns (52,55,96,132,133,144,145) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(file_path, delimiter=\";\", encoding=\"latin1\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"../data/PARAMETROS_TJ2.csv\"\n",
    "\n",
    "# Use latin1 encoding\n",
    "data = pd.read_csv(file_path, delimiter=\";\", encoding=\"latin1\")\n",
    "\n",
    "# Display the first few rows\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we sort the PARAMETROS_TJ2.csv file by the N_DESCARGA field and save the sorted data to a new file named PARAMETROS_TJ2_ORDENADOS.csv. This ensures the rows are ordered starting from the smallest N_DESCARGA value to the largest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b6/pb_stfjn0pb7pybgwlqjk5700000gn/T/ipykernel_87239/1307042932.py:8: DtypeWarning: Columns (52,55,96,132,133,144,145) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(input_file_path, delimiter=\";\", encoding=\"latin1\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted data saved to ../data/PARAMETROS_TJ2_ORDENADOS.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "input_file_path = \"../data/PARAMETROS_TJ2.csv\"  # Replace with the correct path\n",
    "output_file_path = \"../data/PARAMETROS_TJ2_ORDENADOS.csv\"\n",
    "\n",
    "# Read the data\n",
    "data = pd.read_csv(input_file_path, delimiter=\";\", encoding=\"latin1\")\n",
    "\n",
    "# Convert N_DESCARGA to numeric, coercing errors to NaN\n",
    "data['N_DESCARGA'] = pd.to_numeric(data['N_DESCARGA'], errors='coerce')\n",
    "\n",
    "# Drop rows where N_DESCARGA is NaN (invalid values)\n",
    "data = data.dropna(subset=['N_DESCARGA'])\n",
    "\n",
    "# Ensure N_DESCARGA is an integer\n",
    "data['N_DESCARGA'] = data['N_DESCARGA'].astype(int)\n",
    "\n",
    "# Sort by N_DESCARGA in ascending order\n",
    "data_sorted = data.sort_values(by='N_DESCARGA')\n",
    "\n",
    "# Save the sorted dataset to a new CSV file\n",
    "data_sorted.to_csv(output_file_path, index=False, sep=\";\", encoding=\"latin1\")\n",
    "\n",
    "print(f\"Sorted data saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will convert this .csv file to a .txt to help with training with a RAG system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODIFY THE TEXT BELOW TO INCLUDE ONLY THE IMPORTANT FIELDS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b6/pb_stfjn0pb7pybgwlqjk5700000gn/T/ipykernel_87239/1245340582.py:16: DtypeWarning: Columns (52,54,55,96,105,108,110,122,125,129,132,133,137,138,143,144,145,150,151,154) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(csv_path, delimiter=';', encoding='latin1', on_bad_lines='skip')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to ../data/txt_output/PARAMETROS_TJ2_ORDENADOS.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def process_csv_to_txt(csv_path, txt_path):\n",
    "    \"\"\"\n",
    "    Processes a CSV file with semicolon delimiters, extracts and cleans relevant data,\n",
    "    and saves it as a .txt file.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Path to the input CSV file.\n",
    "        txt_path (str): Path to save the output .txt file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the CSV file with semicolon as the delimiter\n",
    "        data = pd.read_csv(csv_path, delimiter=';', encoding='latin1', on_bad_lines='skip')\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {csv_path}: {e}\")\n",
    "        return\n",
    "\n",
    "    # Initialize the text content\n",
    "    text = ''\n",
    "\n",
    "    # Iterate over the rows of the CSV\n",
    "    for index, row in data.iterrows():\n",
    "        # Extract the necessary fields (customize this based on your analysis needs)\n",
    "        descarga = row.get('N_DESCARGA', 'N/A')\n",
    "        fecha = row.get('fecha', 'N/A')\n",
    "        comentario = row.get('comentarioDesc', 'N/A')\n",
    "        configuracion = row.get('configuracion', 'N/A')\n",
    "        energia_diamagnetica = row.get('energia_diamagnetica', 'N/A')\n",
    "\n",
    "        # Format the extracted data into a readable string\n",
    "        text += (\n",
    "            f\"Descarga: {descarga}\\n\"\n",
    "            f\"Fecha: {fecha}\\n\"\n",
    "            f\"Comentario: {comentario}\\n\"\n",
    "            f\"Configuración: {configuracion}\\n\"\n",
    "            f\"Energía Diamagnética: {energia_diamagnetica}\\n\"\n",
    "            f\"{'-'*40}\\n\"  # Separator for readability\n",
    "        )\n",
    "\n",
    "    # Save the processed text to the output file\n",
    "    try:\n",
    "        with open(txt_path, 'w', encoding='utf-8') as txt_file:\n",
    "            txt_file.write(text)\n",
    "        print(f\"Processed data saved to {txt_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to {txt_path}: {e}\")\n",
    "\n",
    "\n",
    "# Input and output paths\n",
    "csv_path = \"../data/PARAMETROS_TJ2_ORDENADOS.csv\"\n",
    "txt_folder = \"../data/txt_output/\"\n",
    "\n",
    "# Ensure the output folder exists\n",
    "os.makedirs(txt_folder, exist_ok=True)\n",
    "\n",
    "# Generate the output .txt file name\n",
    "txt_path = os.path.join(txt_folder, \"PARAMETROS_TJ2_ORDENADOS.txt\")\n",
    "\n",
    "# Process the CSV and save it as a .txt file\n",
    "process_csv_to_txt(csv_path, txt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay que borrar los datos repetidos, hay muchos donde la hora y la fecha son iguales, aunque el numero de descarga sea diferente, así que habría que dejar solo uno. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora hay que crear una base de datos vectorial a partir de los archivos .txt (database_creator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the code is using a GPU if one is available, and it’s enabled by the SentenceTransformer library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRY THIS IN GOOGLE COLAB TO CREATE THE VECTOR_DATABASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Import Libraries\n",
    "import json\n",
    "import os\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Ensure CUDA configuration for PyTorch\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Step 3: Define Helper Functions\n",
    "# Load and split the text file into paragraphs\n",
    "def load_and_split_text(file_path, context_size=0):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Split the text into paragraphs based on double line breaks\n",
    "    split_paragraphs = [para.strip() for para in text.split('\\n\\n') if para.strip()]\n",
    "\n",
    "    paragraphs = []\n",
    "    for para in split_paragraphs:\n",
    "        # Handle missing or invalid values\n",
    "        para = para.replace(\"nan\", \"N/A\")  # Replace 'nan' with a placeholder\n",
    "        paragraphs.append(para)\n",
    "\n",
    "    # Add context if needed\n",
    "    if context_size > 0:\n",
    "        paragraphs_with_context = []\n",
    "        for i, para in enumerate(paragraphs):\n",
    "            previous_context = paragraphs[i - 1][-context_size:] if i > 0 else \"\"\n",
    "            next_context = paragraphs[i + 1][:context_size] if i < len(paragraphs) - 1 else \"\"\n",
    "            combined_para = f\"{previous_context} {para} {next_context}\"\n",
    "            paragraphs_with_context.append(combined_para)\n",
    "        return paragraphs_with_context\n",
    "\n",
    "    return paragraphs\n",
    "\n",
    "\n",
    "# Generate embeddings for the paragraphs\n",
    "def generate_embeddings(paragraphs, model_name):\n",
    "    model = SentenceTransformer(model_name, device='cuda')\n",
    "    metadata = []\n",
    "\n",
    "    for i, para in enumerate(paragraphs):\n",
    "        metadata.append({\n",
    "            \"id\": i,\n",
    "            \"paragraph\": para,\n",
    "        })\n",
    "    embeddings = model.encode(paragraphs, convert_to_numpy=True)\n",
    "    return embeddings, metadata\n",
    "\n",
    "\n",
    "# Build a FAISS index\n",
    "def build_faiss_index(embeddings):\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "\n",
    "# Main processing for the nuclear fusion project\n",
    "def process_nuclear_fusion_data(txt_path, model_name, context_sizes, database_base_path):\n",
    "    for context_size in context_sizes:\n",
    "        database_path = os.path.join(database_base_path, f\"{model_name}_{context_size}\")\n",
    "\n",
    "        index_path = os.path.join(database_path, \"db.index\")\n",
    "        metadata_path = os.path.join(database_path, \"metadata.json\")\n",
    "\n",
    "        print(f\"Processing with context size {context_size}...\")\n",
    "\n",
    "        # Load and split the text\n",
    "        print(\"Loading and splitting text...\")\n",
    "        paragraphs = load_and_split_text(txt_path, context_size=context_size)\n",
    "\n",
    "        # Generate embeddings\n",
    "        print(\"Generating embeddings...\")\n",
    "        embeddings, metadata = generate_embeddings(paragraphs, model_name=model_name)\n",
    "\n",
    "        # Build FAISS index\n",
    "        print(\"Building FAISS index...\")\n",
    "        index = build_faiss_index(embeddings)\n",
    "\n",
    "        # Save the database\n",
    "        print(f\"Saving the database to {database_path}...\")\n",
    "        if not os.path.exists(database_path):\n",
    "            os.makedirs(database_path, exist_ok=True)\n",
    "        faiss.write_index(index, index_path)\n",
    "\n",
    "        # Save the metadata as a JSON file\n",
    "        with open(metadata_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(metadata, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(\"Database creation complete.\")\n",
    "\n",
    "# Step 4: Paths and Configurations\n",
    "TXT_FILE_PATH = \"/content/PARAMETROS_TJ2_ORDENADOS.txt\"  # Absolute path to your file\n",
    "MODEL_NAME = \"allenai/scibert_scivocab_uncased\"  # Use a suitable pre-trained model\n",
    "CONTEXT_SIZES = [0, 25, 50, 75, 100]  # Adjust context sizes as needed\n",
    "DATABASE_BASE_PATH = \"./vector_database\"  # Local path in Colab environment\n",
    "\n",
    "# Step 5: Process the Text File and Create the Vectorial Database\n",
    "process_nuclear_fusion_data(TXT_FILE_PATH, MODEL_NAME, CONTEXT_SIZES, DATABASE_BASE_PATH)\n",
    "\n",
    "# Step 6: Download Output Files\n",
    "# Zip the output folder for download\n",
    "!zip -r vector_database.zip ./vector_database\n",
    "\n",
    "# Provide a link to download\n",
    "from google.colab import files\n",
    "files.download('vector_database.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV has been successfully converted to JSON and saved to ../data/PARAMETROS_TJ2_model.json\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Path to the input CSV file\n",
    "csv_file_path = \"../data/PARAMETROS_TJ2_model.csv\"\n",
    "# Path to the output JSON file\n",
    "json_file_path = \"../data/PARAMETROS_TJ2_model.json\"\n",
    "\n",
    "# Open and process the CSV file\n",
    "with open(csv_file_path, mode='r', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=';')  # Adjust delimiter based on your file\n",
    "    data = []\n",
    "\n",
    "    # Read each row and convert to OrderedDict to maintain order\n",
    "    for row in csv_reader:\n",
    "        ordered_row = OrderedDict(row)\n",
    "        data.append(ordered_row)\n",
    "\n",
    "# Write the data to a JSON file\n",
    "with open(json_file_path, mode='w', encoding='utf-8') as json_file:\n",
    "    json.dump(data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"CSV has been successfully converted to JSON and saved to {json_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write a script so that it goes through the json file, and so that if the value is empty, it deletes it. e.g., here it should detele this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned JSON has been saved to ../data/PARAMETROS_TJ2_model_cleaned.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Path to the input JSON file\n",
    "json_file_path = \"../data/PARAMETROS_TJ2_model.json\"\n",
    "# Path to the output cleaned JSON file\n",
    "cleaned_json_file_path = \"../data/PARAMETROS_TJ2_model_cleaned.json\"\n",
    "\n",
    "# Open and load the JSON file\n",
    "with open(json_file_path, mode='r', encoding='utf-8') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Function to clean empty values from a dictionary\n",
    "def clean_empty_values(item):\n",
    "    if isinstance(item, dict):\n",
    "        return {k: clean_empty_values(v) for k, v in item.items() if v not in [None, \"\", []]}\n",
    "    elif isinstance(item, list):\n",
    "        return [clean_empty_values(i) for i in item if i not in [None, \"\", []]]\n",
    "    else:\n",
    "        return item\n",
    "\n",
    "# Clean the data\n",
    "cleaned_data = clean_empty_values(data)\n",
    "\n",
    "# Save the cleaned data back to a JSON file\n",
    "with open(cleaned_json_file_path, mode='w', encoding='utf-8') as cleaned_json_file:\n",
    "    json.dump(cleaned_data, cleaned_json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Cleaned JSON has been saved to {cleaned_json_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_DESCARGA values where any specified key is not 0:\n",
      "['8520', '8521', '8523', '8524', '8525', '8526', '8527', '8528', '8529', '8530', '8531', '8532', '8533', '8534', '8536', '8537', '8538', '8539', '8540', '8541', '8542', '8543', '8544', '8545', '8546', '8547', '8548', '8549', '8550', '8551', '8552', '8553', '8554', '8555', '8556', '8557', '8558', '8559', '8560', '8561', '8562', '8672', '8673', '8674', '8675', '8676', '8677', '8678', '8679', '8688', '8689', '8717', '8718', '8719', '8720', '8721', '9414', '9422', '9423', '9424', '9426', '9427', '9428', '9429', '9430', '9431', '9432', '9433', '9434', '9435', '9436', '9437', '9438', '9439', '9440', '9441', '9442', '9443', '9444', '9445', '9446', '9447', '9448', '9620', '9621', '9622', '9623', '9624', '9833', '9834', '9835', '9836', '9837', '9888', '9889', '9891', '9892', '9893', '9894', '9895', '9898', '9899', '9900', '9901', '9902', '9903', '9904', '9905', '9906', '9907', '9908', '9909', '11669', '11670', '11671', '11672', '14000', '14001', '14002', '14003', '14004', '14005', '14006', '14007', '14008', '14971', '14972', '14973', '14974', '14975', '14976', '14977', '14978', '14979', '14980', '14981', '14982', '14983', '14984', '14985', '14998', '14999', '15000', '15001', '15002', '15003', '15004', '15005', '15006', '15007', '15008', '15009', '15010', '15011', '15012', '15013', '15014', '15015', '15016', '15017', '15018', '15019', '15020', '15021', '15022', '15023', '15024', '15025', '15026', '15027', '15028', '15029', '15030', '15031', '15032', '15402', '15403', '15404', '15405', '15406', '15407', '15415', '15416', '15417', '15418', '15419', '15420', '15421', '15770', '15771', '15772', '15773', '15774', '15775', '16805', '16806', '16807', '16808', '16809', '16810', '16811', '16812', '16813', '16814', '16815', '16816', '16817', '16818', '16821', '16822', '16823', '16824', '16825', '16828', '16829', '16830', '16831', '16832', '16833', '17889', '17890', '17891', '17892', '17893', '17894', '17895', '17896', '17897', '17898', '19872', '19873', '19874', '19875', '19876', '19877', '19878', '19879', '19880', '19881', '19882', '19883', '19884', '19885', '19886', '19887', '19888', '19889', '24382', '24383', '24384', '24385', '24386', '24387', '24388', '24389', '24390', '24391', '24392', '24393', '24394', '24395', '24396', '24397', '24398', '24399', '24400', '24401', '24402', '24403', '24404', '24405', '24406', '24407', '24408', '24409', '24410', '24411', '24412', '24413', '24414', '24415', '24416', '24417', '24418', '24419', '24420', '24421', '24422', '24423', '24424', '24425', '24426', '24427', '24428', '24429', '24430', '24431', '24432', '24433', '24434', '24435', '24436', '24437', '24438', '24439', '24440', '24441', '24442', '24443', '26846', '26847', '26848', '26849', '26850', '26851', '26852', '26853', '26854', '26855', '26856', '26857', '26858', '29873', '29874', '29875', '29876', '29877', '29878', '29880', '29881', '29882', '29883', '29884', '29885', '29886', '29887', '29888', '29889', '39760', '39761', '39762', '39763', '39764', '41410', '41411', '41412', '41422', '41423', '41424', '41425', '41426', '41427', '41428', '41429', '41430', '41431', '41432', '41433', '41434', '41435', '41436', '41437', '41438', '41439', '41440', '41441', '41442', '41443', '41444', '41445', '41446', '41447', '41448', '41458', '41459', '41460', '41461', '41468', '41469', '41470', '41471', '41472', '41473', '41474', '41475', '41479', '41480', '41481', '41482', '41483', '41484', '41485', '41486', '41487', '41488', '41489', '41490', '41491', '41492', '41493', '41494', '41495', '41496', '42508', '42509', '42510', '42511', '42512', '42513', '42514', '42515', '42516', '42517', '42518', '42519', '42813', '42814', '42815', '42816', '42825', '42826', '42827', '42828', '42829', '42830', '42831', '42832', '42833', '42834', '42835', '42836', '42837', '42838', '42839', '42840', '42841', '42842', '42843', '42844', '42849', '42850', '42851', '42852', '43009', '43010', '43011', '43012', '43013', '43014', '43015', '43016', '43017', '43018', '43019', '43020', '43021', '43022', '43023', '43024', '43025', '43026', '44268', '44269', '44270', '44271', '44294', '46091', '46092', '46093', '46094', '46095', '46096', '46850', '46851', '46852', '46853', '47110', '47111', '47112', '47117', '47118', '47119', '47120', '47121', '47122', '47123', '47124', '47125', '47128', '47129', '47130', '47131', '47132', '47133', '47134', '47135', '49134', '49135', '49136', '49137', '49138', '49139', '49140', '49141', '49142', '50309', '50310', '50311', '50312', '50313', '50314', '50760', '50761', '50762', '50763', '50831', '50832', '50833', '50848', '50849', '50850', '50851', '50852', '50853', '50854', '50855', '50856', '50857', '50858', '51214', '51215', '51216', '51217', '51218', '52277', '52278', '52279', '52280', '52281', '52827', '52828', '52829', '52830', '52831', '52832', '52864', '52865', '52866', '52867', '52868', '56377', '56378', '56379', '57241', '57242', '57243', '57244', '57245', '57246', '57247']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Path to the input JSON file\n",
    "json_file_path = \"../data/PARAMETROS_TJ2_model_cleaned.json\"\n",
    "\n",
    "# List of keys to check\n",
    "keys_to_check = [\n",
    "    \"valvula_li1\", \"valvula_li2\", \"valvula_he\", \"valvula_cx1\", \"valvula_cx2\", \n",
    "    \"valvula_nb\", \"valvula_cnb1\", \"valvula_cnb2\", \n",
    "    \"polaridad_limitador_a3bot\", \"polaridad_limitador_c3bot\", \"polaridad_electrodo_a7top\"\n",
    "]\n",
    "\n",
    "# Open and load the JSON file\n",
    "with open(json_file_path, mode='r', encoding='utf-8') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Find N_DESCARGA values where any of the keys have a value not equal to 0\n",
    "n_descarga_values = [\n",
    "    item[\"N_DESCARGA\"] for item in data\n",
    "    if any(key in item and item[key] not in [\"0\", 0, None] for key in keys_to_check)\n",
    "]\n",
    "\n",
    "# Output the results\n",
    "print(\"N_DESCARGA values where any specified key is not 0:\")\n",
    "print(n_descarga_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace all valvula_* keys with a single valvula = 0 and all polaridad_* keys with polaridad = 0 in the JSON file. This simplifies the structure and reduces redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced JSON has been saved to ../data/PARAMETROS_TJ2_model_reduced.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Path to the input JSON file\n",
    "json_file_path = \"../data/PARAMETROS_TJ2_model_cleaned.json\"\n",
    "# Path to the output JSON file\n",
    "output_json_file_path = \"../data/PARAMETROS_TJ2_model_reduced.json\"\n",
    "\n",
    "# Lists of keys to consolidate\n",
    "valvula_keys = [\n",
    "    \"valvula_li1\", \"valvula_li2\", \"valvula_he\", \"valvula_cx1\", \"valvula_cx2\",\n",
    "    \"valvula_nb\", \"valvula_cnb1\", \"valvula_cnb2\"\n",
    "]\n",
    "polaridad_keys = [\n",
    "    \"polaridad_limitador_a3bot\", \"polaridad_limitador_c3bot\", \"polaridad_electrodo_a7top\"\n",
    "]\n",
    "\n",
    "# Open and load the JSON file\n",
    "with open(json_file_path, mode='r', encoding='utf-8') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Update each item in the JSON data\n",
    "for item in data:\n",
    "    # Replace all valvula_* keys with valvula = 0\n",
    "    if all(item.get(key) in [\"0\", 0, None] for key in valvula_keys):\n",
    "        # Remove individual valvula_* keys\n",
    "        for key in valvula_keys:\n",
    "            item.pop(key, None)\n",
    "        # Add consolidated valvula = 0\n",
    "        item[\"valvula\"] = \"0\"\n",
    "\n",
    "    # Replace all polaridad_* keys with polaridad = 0\n",
    "    if all(item.get(key) in [\"0\", 0, None] for key in polaridad_keys):\n",
    "        # Remove individual polaridad_* keys\n",
    "        for key in polaridad_keys:\n",
    "            item.pop(key, None)\n",
    "        # Add consolidated polaridad = 0\n",
    "        item[\"polaridad\"] = \"0\"\n",
    "\n",
    "# Save the updated JSON data to a new file\n",
    "with open(output_json_file_path, mode='w', encoding='utf-8') as output_json_file:\n",
    "    json.dump(data, output_json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Reduced JSON has been saved to {output_json_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_task_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
